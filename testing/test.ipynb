{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just be taking a sample model, taken with help of gpt and have a quick glance whether the model is correct or not. Since my main focus is checking whether my implementation is correct or not\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import  qlora_and_lora  as Q\n",
    "from Q import quantize_model, Qlora_4_bit\n",
    "# 1. Load a small, real model from Hugging Face\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "print(f\"Loading {model_id}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\", # Load to CPU first for safety\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 2. Inspect the original structure to find target layer names\n",
    "print(\"\\n--- Original Layer Structure (First Block) ---\")\n",
    "print(model.model.layers[0].self_attn)\n",
    "# You will see names like: 'q_proj', 'k_proj', 'v_proj', 'o_proj'\n",
    "\n",
    "# 3. Define the target layers you want to convert\n",
    "# For Llama models, typical targets are q_proj and v_proj\n",
    "target_layers = [\"q_proj\", \"v_proj\", \"o_proj\", \"gate_proj\",\"up_gate\", \"down_gate\", \"k_proj\", \"up_proj\", \"down_proj\"] # check using name, _ in model.named_modules\n",
    "\n",
    "# 4. Apply YOUR quantize_model function\n",
    "print(f\"\\nApplying QLoRA to layers: {target_layers}...\")\n",
    "\n",
    "# (Assuming your classes Qlora_4_bit and quantize_model are defined above)\n",
    "quantize_model(\n",
    "    module=model,\n",
    "    mlp_layers=target_layers,\n",
    "    alpha=4,\n",
    "    r=1,\n",
    "    require_lora=True,\n",
    "    use_qlora=True # Set False for CPU testing, True if you have GPU+BitsAndBytes setup\n",
    ")\n",
    "\n",
    "# 5. VERIFICATION 1: Check if layers changed\n",
    "print(\"\\n--- Modified Layer Structure (First Block) ---\")\n",
    "# Access the first layer to see if it is now Qlora_4_bit\n",
    "print(model.model.layers[0].self_attn.q_proj)\n",
    "\n",
    "if isinstance(model.model.layers[0].self_attn.q_proj, Qlora_4_bit):\n",
    "    print(\"\\n‚úÖ SUCCESS: Layer successfully replaced with custom QLoRA class!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå FAILURE: Layer is still the original nn.Linear.\")\n",
    "\n",
    "# 6. VERIFICATION 2: The \"Smoke Test\" (Forward Pass)\n",
    "print(\"\\n--- Running Forward Pass (Dimension Check) ---\")\n",
    "dummy_input = torch.randint(0, 31000, (1, 20), device='cpu').long()\n",
    "output = model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "\n",
    "    print(f\"all params: {all_param:,d}\")\n",
    "    print(f\"trainable params: {trainable_params:,d}\")\n",
    "    print(f\"trainable%: {100 * trainable_params / all_param:.4f}%\")\n",
    "\n",
    "# Run it\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "DATASET_ID = \"Abirate/english_quotes\" # Small, fast dataset\n",
    "MAX_STEPS = 60  # Short run to verify loss drop\n",
    "LEARNING_RATE = 2e-4 # High LR for visible results\n",
    "\n",
    "# --- 2. LOAD MODEL & TOKENIZER ---\n",
    "print(f\"Loading {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Fix for Llama/Mistral Tokenizers\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "print(\"Injecting Custom Adapters...\")\n",
    "quantize_model(\n",
    "    module=model,\n",
    "    mlp_layers=target_layers,\n",
    "    alpha=4, #<----- KEEP IT HIGH OTHER WISE YOU WONT SEE ANYTHING TRAINING ALPHA/R ~ 2\n",
    "    r=2,\n",
    "    require_lora=True,\n",
    "    use_qlora=True # use 4 bit module and check for normal nn.Linear\n",
    ")\n",
    "print(\"Locking weights...\")\n",
    "model.model.embed_tokens.weight.requires_grad = False\n",
    "\n",
    "# Verify correct parameter count\n",
    "def print_trainable(m):\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    print(f\"‚úÖ Final Trainable Params: {trainable:,} ({100*trainable/total:.3f}%)\")\n",
    "\n",
    "print_trainable(model)\n",
    "\n",
    "# --- 5. PREPARE DATASET ---\n",
    "print(f\"Loading dataset: {DATASET_ID}...\")\n",
    "data = load_dataset(DATASET_ID)\n",
    "\n",
    "def tokenize(element):\n",
    "    return tokenizer(\n",
    "        element[\"quote\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "tokenized_dataset = data.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=data[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# --- 6. DEFINE TRAINER ---\n",
    "print(\"Setting up Trainer...\")\n",
    "# ALL GPT SINCE I WANNA SEE THE LOSS\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"custom_qlora_outputs\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=5,         # Show loss every 5 steps\n",
    "    max_steps=MAX_STEPS,     # Stop after 60 steps\n",
    "    save_strategy=\"no\",      # Don't save checkpoints during test\n",
    "    optim=\"adamw_torch\",     # Standard optimizer\n",
    "    report_to=\"none\",        # Disable WandB for now\n",
    "    remove_unused_columns=False # Important for custom models\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# --- 7. START TRAINING ---\n",
    "print(\"\\nüöÄ Starting Training! Watch the 'Loss' column...\\n\")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
